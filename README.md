# AI-Powered RAG Assistant Using LangChain and Gradio

This project demonstrates how to build a Retrieval-Augmented Generation (RAG) application using LangChain, IBM WatsonX, and Gradio. The project was completed as part of Coursera's "Generative AI Applications with RAG and LangChain" course.

## 🚀 Features
- Load documents from multiple sources using LangChain loaders
- Apply advanced text splitting strategies for token optimization
- Generate embeddings using IBM WatsonX
- Store and retrieve vectors using ChromaDB
- Implement a semantic retriever for contextual chunk retrieval
- Build a QA bot using LangChain + LLM
- Deploy an interactive interface using Gradio

## 🖼️ Screenshots
All steps were executed within Coursera Labs. Below are screenshots of each module:

- 📄 Document Loader (`pdf_loader.png`)
- ✂️ Text Splitter (`code_splitter.png`)
- 🔢 Embedding Generation (`embedding.png`)
- 🗃️ Vector DB Setup (`vectordb.png`)
- 🔍 Retriever Setup (`retriever.png`)
- 🤖 QA Bot Interface (`QA_bot.png`)

## 🛠️ Tech Stack
- LangChain
- IBM WatsonX Embeddings
- ChromaDB
- Gradio
- Python

## 📚 Course
Completed as part of [Generative AI Applications with RAG and LangChain](https://www.coursera.org/learn/langchain-rag) on Coursera

## 📌 Note
This repository reflects a reproduction of work completed on the Coursera lab environment.
