# AI-Powered RAG Assistant Using LangChain and Gradio

This project demonstrates how to build a Retrieval-Augmented Generation (RAG) application using LangChain, IBM WatsonX, and Gradio. The project was completed as part of Coursera's "Generative AI Applications with RAG and LangChain" course.

## ğŸš€ Features
- Load documents from multiple sources using LangChain loaders
- Apply advanced text splitting strategies for token optimization
- Generate embeddings using IBM WatsonX
- Store and retrieve vectors using ChromaDB
- Implement a semantic retriever for contextual chunk retrieval
- Build a QA bot using LangChain + LLM
- Deploy an interactive interface using Gradio

## ğŸ–¼ï¸ Screenshots
All steps were executed within Coursera Labs. Below are screenshots of each module:

- ğŸ“„ Document Loader (`pdf_loader.png`)
- âœ‚ï¸ Text Splitter (`code_splitter.png`)
- ğŸ”¢ Embedding Generation (`embedding.png`)
- ğŸ—ƒï¸ Vector DB Setup (`vectordb.png`)
- ğŸ” Retriever Setup (`retriever.png`)
- ğŸ¤– QA Bot Interface (`QA_bot.png`)

## ğŸ› ï¸ Tech Stack
- LangChain
- IBM WatsonX Embeddings
- ChromaDB
- Gradio
- Python

## ğŸ“š Course
Completed as part of [Generative AI Applications with RAG and LangChain](https://www.coursera.org/learn/langchain-rag) on Coursera

## ğŸ“Œ Note
This repository reflects a reproduction of work completed on the Coursera lab environment.
